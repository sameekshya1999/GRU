{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNo0PhU9Jg1dVvhUc46zg6m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameekshya1999/GRU/blob/main/transformer_one_nextword.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4X3F-QocOm8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import textwrap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def pretty_print(s):\n",
        "    print(\"Output:\\n\" + 80 * '-')\n",
        "    print(textwrap.fill(tokenizer.decode(s, skip_special_tokens=True), 80))\n",
        "\n",
        "model_to_use = \"gpt2-large\"\n",
        "\n",
        "print(\"Using model: \", model_to_use)\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_to_use)\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(model_to_use,\n",
        "                                        output_scores=True,\n",
        "                                        pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "print(model.config)\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset  # Ensure `datasets` library is installed\n",
        "\n",
        "# Load the dataset (replace with your dataset)\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# Set a padding token for the tokenizer\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Tokenize the dataset\n",
        "texts = dataset[\"text\"]\n",
        "tokenized_texts = tokenizer(\"\\n\\n\".join(texts), return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "# Prepare input-output pairs for next-word prediction\n",
        "seq_length = 50  # Define sequence length\n",
        "input_ids = tokenized_texts[\"input_ids\"][0]\n",
        "\n",
        "X, y = [], []\n",
        "for i in range(seq_length, len(input_ids)):\n",
        "    X.append(input_ids[i-seq_length:i])  # Previous tokens\n",
        "    y.append(input_ids[i])  # Target token (next word)\n",
        "\n",
        "# Split data into training (70%), validation (15%), and test (15%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)  # 70% training, 30% temporary\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # Split 30% into 15% val and 15% test\n",
        "\n",
        "# Convert sequences to tensors and pad them\n",
        "X_train = pad_sequence([torch.tensor(seq) for seq in X_train], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "X_val = pad_sequence([torch.tensor(seq) for seq in X_val], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "X_test = pad_sequence([torch.tensor(seq) for seq in X_test], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "y_train = torch.tensor(y_train)\n",
        "y_val = torch.tensor(y_val)\n",
        "y_test = torch.tensor(y_test)\n",
        "\n",
        "# Print dataset sizes for confirmation\n",
        "print(f\"Training set: {len(X_train)} samples\")\n",
        "print(f\"Validation set: {len(X_val)} samples\")\n",
        "print(f\"Test set: {len(X_test)} samples\")\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Define loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "epochs = 3\n",
        "batch_size = 32\n",
        "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    # Training step\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        batch_X = X_train[i:i+batch_size]\n",
        "        batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X, labels=batch_X)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits[:, -1, :]  # Logits of the last token\n",
        "        predictions = logits.argmax(dim=-1)\n",
        "\n",
        "        # Accuracy calculation\n",
        "        correct += (predictions == batch_y).sum().item()\n",
        "        total += batch_y.size(0)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    train_losses.append(epoch_loss / total)\n",
        "    train_accs.append(correct / total)\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(X_val), batch_size):\n",
        "            batch_X = X_val[i:i+batch_size]\n",
        "            batch_y = y_val[i:i+batch_size]\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(batch_X, labels=batch_X)\n",
        "            val_loss += outputs.loss.item()\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            predictions = logits.argmax(dim=-1)\n",
        "\n",
        "            val_correct += (predictions == batch_y).sum().item()\n",
        "            val_total += batch_y.size(0)\n",
        "\n",
        "    val_losses.append(val_loss / val_total)\n",
        "    val_accs.append(val_correct / val_total)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, \"\n",
        "          f\"Val Loss: {val_losses[-1]:.4f}, Train Acc: {train_accs[-1]:.4f}, \"\n",
        "          f\"Val Acc: {val_accs[-1]:.4f}\")\n",
        "\n",
        "# Test evaluation after training\n",
        "model.eval()\n",
        "test_loss, test_correct, test_total = 0, 0, 0\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(X_test), batch_size):\n",
        "        batch_X = X_test[i:i+batch_size]\n",
        "        batch_y = y_test[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X, labels=batch_X)\n",
        "        test_loss += outputs.loss.item()\n",
        "        logits = outputs.logits[:, -1, :]\n",
        "        predictions = logits.argmax(dim=-1)\n",
        "\n",
        "        test_correct += (predictions == batch_y).sum().item()\n",
        "        test_total += batch_y.size(0)\n",
        "\n",
        "# Calculate final test metrics\n",
        "test_loss /= test_total\n",
        "test_accuracy = test_correct / test_total\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming test_loss and test_accuracy are calculated after the training loop\n",
        "test_loss_value = test_loss  # Test loss from evaluation\n",
        "test_accuracy_value = test_accuracy  # Test accuracy from evaluation\n",
        "\n",
        "# Plot Loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
        "plt.plot(val_losses, label=\"Validation Loss\", marker='o')\n",
        "plt.axhline(y=test_loss_value, color='r', linestyle='--', label=\"Test Loss\")  # Add test loss as a horizontal line\n",
        "plt.title(\"Loss vs. Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_accs, label=\"Train Accuracy\", marker='o')\n",
        "plt.plot(val_accs, label=\"Validation Accuracy\", marker='o')\n",
        "plt.axhline(y=test_accuracy_value, color='r', linestyle='--', label=\"Test Accuracy\")  # Add test accuracy as a horizontal line\n",
        "plt.title(\"Accuracy vs. Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ]
}